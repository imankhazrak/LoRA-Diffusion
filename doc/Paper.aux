\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{brown2020language}
\citation{hu2021lora}
\citation{lou2023discrete,sahoo2024masked}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{austin2021structured}
\citation{hoogeboom2021autoregressive}
\citation{lou2023discrete}
\citation{sahoo2024masked}
\citation{li2022diffusion}
\citation{hu2021lora}
\citation{dettmers2023qlora}
\citation{zhang2023adalora}
\citation{li2021prefix}
\citation{lester2021power}
\citation{houlsby2019parameter}
\citation{zaken2021bitfit}
\citation{ilharco2022editing}
\citation{wang2020orthogonal}
\citation{fedus2022switch}
\citation{aghajanyan2020intrinsic}
\citation{li2018measuring}
\citation{tishby2015deep}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Diffusion Models for Language}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Parameter-Efficient Fine-Tuning}{2}{subsection.2.2}\protected@file@percent }
\citation{austin2021structured}
\citation{hu2021lora}
\citation{aghajanyan2020intrinsic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Multi-Task Learning and Low-Rank Theory}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\newlabel{sec:method}{{3}{3}{Methodology}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminaries}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Trajectory-Level Low-Rank Adaptation}{3}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Training Objective}{4}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Multi-Task Composition}{4}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Inference Procedure}{4}{subsection.3.5}\protected@file@percent }
\citation{lou2023discrete}
\citation{tishby2000information}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces LoRA-Diffusion Inference\relax }}{5}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:inference}{{1}{5}{LoRA-Diffusion Inference\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Implementation Details}{5}{subsection.3.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Base model configurations.\relax }}{5}{table.caption.1}\protected@file@percent }
\newlabel{tab:model_configs}{{1}{5}{Base model configurations.\relax }{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Theoretical Justification}{5}{subsection.3.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces LoRA-Diffusion hyperparameters.\relax }}{6}{table.caption.2}\protected@file@percent }
\newlabel{tab:lora_hyperparams}{{2}{6}{LoRA-Diffusion hyperparameters.\relax }{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of parameter-efficient fine-tuning methods.\relax }}{6}{table.caption.3}\protected@file@percent }
\newlabel{tab:peft_comparison}{{3}{6}{Comparison of parameter-efficient fine-tuning methods.\relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Conceptual comparison: Weight LoRA vs.\ LoRA-Diffusion.\relax }}{6}{table.caption.4}\protected@file@percent }
\newlabel{tab:theory_comparison}{{4}{6}{Conceptual comparison: Weight LoRA vs.\ LoRA-Diffusion.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments and Results}{6}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{6}{Experiments and Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experimental Setup}{6}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Main Results}{7}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Average performance on SST-2 (1.3B model).\relax }}{7}{table.caption.5}\protected@file@percent }
\newlabel{tab:main_results}{{5}{7}{Average performance on SST-2 (1.3B model).\relax }{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Detailed results on SST-2 sentiment classification.\relax }}{7}{table.caption.6}\protected@file@percent }
\newlabel{tab:per_task_results}{{6}{7}{Detailed results on SST-2 sentiment classification.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Efficiency Analysis}{7}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Training and inference efficiency on SST-2 (1.3B model, batch size 64).\relax }}{7}{table.caption.7}\protected@file@percent }
\newlabel{tab:efficiency}{{7}{7}{Training and inference efficiency on SST-2 (1.3B model, batch size 64).\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Catastrophic Forgetting and Convergence}{8}{subsection.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Training loss and convergence on SST-2 (lower loss is better).\relax }}{8}{table.caption.8}\protected@file@percent }
\newlabel{tab:forgetting}{{8}{8}{Training loss and convergence on SST-2 (lower loss is better).\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Method Comparison Summary}{8}{subsection.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Method comparison summary on SST-2.\relax }}{8}{table.caption.9}\protected@file@percent }
\newlabel{tab:composition}{{9}{8}{Method comparison summary on SST-2.\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Rank and Module Ablations}{8}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Model Size Scaling}{8}{subsection.4.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Rank configuration ablation (SST-2).\relax }}{9}{table.caption.10}\protected@file@percent }
\newlabel{tab:rank_ablation}{{10}{9}{Rank configuration ablation (SST-2).\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Effect of number of LoRA modules per step.\relax }}{9}{table.caption.11}\protected@file@percent }
\newlabel{tab:num_modules}{{11}{9}{Effect of number of LoRA modules per step.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Trajectory vs.\ Weight LoRA}{9}{subsection.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Placeholder Figures}{9}{subsection.4.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Rank vs.\ performance and vs.\ trainable parameters. Step-adaptive ranks (8/32/64) match uniform $r=64$ with fewer parameters. Generate from experiment logs.\relax }}{9}{figure.caption.14}\protected@file@percent }
\newlabel{fig:rank_ablation}{{1}{9}{Rank vs.\ performance and vs.\ trainable parameters. Step-adaptive ranks (8/32/64) match uniform $r=64$ with fewer parameters. Generate from experiment logs.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{9}{Conclusion}{section.5}{}}
\bibstyle{plainnat}
\bibdata{reference}
\bibcite{aghajanyan2020intrinsic}{{1}{2020}{{Aghajanyan et~al.}}{{Aghajanyan, Zettlemoyer, and Gupta}}}
\bibcite{austin2021structured}{{2}{2021}{{Austin et~al.}}{{Austin, Johnson, Ho, Tarlow, and {Van Den Berg}}}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Performance vs.\ model size (illustrative).\relax }}{10}{table.caption.12}\protected@file@percent }
\newlabel{tab:model_scaling}{{12}{10}{Performance vs.\ model size (illustrative).\relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Trajectory LoRA vs.\ weight LoRA.\relax }}{10}{table.caption.13}\protected@file@percent }
\newlabel{tab:detailed_comparison}{{13}{10}{Trajectory LoRA vs.\ weight LoRA.\relax }{table.caption.13}{}}
\bibcite{brown2020language}{{3}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\bibcite{dettmers2023qlora}{{4}{2023}{{Dettmers et~al.}}{{Dettmers, Pagnoni, Holtzman, and Zettlemoyer}}}
\bibcite{fedus2022switch}{{5}{2022}{{Fedus et~al.}}{{Fedus, Zoph, and Shazeer}}}
\bibcite{hoogeboom2021autoregressive}{{6}{2021}{{Hoogeboom et~al.}}{{Hoogeboom, Nielsen, Jaini, Forr{\'e}, and Welling}}}
\bibcite{houlsby2019parameter}{{7}{2019}{{Houlsby et~al.}}{{Houlsby, Giurgiu, Jastrz{\k {e}}bski, Morrone, {De Laroussilhe}, Gesmundo, Attariyan, and Gelly}}}
\bibcite{hu2021lora}{{8}{2021}{{Hu et~al.}}{{Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}}}
\bibcite{ilharco2022editing}{{9}{2022}{{Ilharco et~al.}}{{Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt, Hajishirzi, and Farhadi}}}
\bibcite{lester2021power}{{10}{2021}{{Lester et~al.}}{{Lester, Al-Rfou, and Constant}}}
\bibcite{li2018measuring}{{11}{2018}{{Li et~al.}}{{Li, Farkhoor, Liu, and Yosinski}}}
\bibcite{li2021prefix}{{12}{2021}{{Li and Liang}}{{}}}
\bibcite{li2022diffusion}{{13}{2022}{{Li et~al.}}{{Li, Thickstun, Gulrajani, Liang, and Hashimoto}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Effective rank of LoRA modules across diffusion steps. Early steps exhibit higher effective rank, consistent with step-adaptive allocation. Generate from experiment logs.\relax }}{11}{figure.caption.15}\protected@file@percent }
\newlabel{fig:effective_rank}{{2}{11}{Effective rank of LoRA modules across diffusion steps. Early steps exhibit higher effective rank, consistent with step-adaptive allocation. Generate from experiment logs.\relax }{figure.caption.15}{}}
\bibcite{lou2023discrete}{{14}{2023}{{Lou et~al.}}{{Lou, Meng, and Ermon}}}
\bibcite{sahoo2024masked}{{15}{2024}{{Sahoo et~al.}}{{Sahoo, Nguyen, Loh, Kumar, and Narasimhan}}}
\bibcite{tishby2015deep}{{16}{2015}{{Tishby and Zaslavsky}}{{}}}
\bibcite{tishby2000information}{{17}{2000}{{Tishby et~al.}}{{Tishby, Pereira, and Bialek}}}
\bibcite{wang2020orthogonal}{{18}{2020}{{Wang et~al.}}{{Wang, Zhang, Lee, Zhang, Sun, Ren, Su, Perot, Dy, and Pfister}}}
\bibcite{zaken2021bitfit}{{19}{2021}{{Zaken et~al.}}{{Zaken, Ravfogel, and Goldberg}}}
\bibcite{zhang2023adalora}{{20}{2023}{{Zhang et~al.}}{{Zhang, Chen, Bukharin, He, Cheng, Chen, and Zhao}}}
\gdef \@abspage@last{12}
