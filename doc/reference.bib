@article{aghajanyan2020intrinsic,
  author    = {Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  title     = {Intrinsic dimensionality explains the effectiveness of language model fine-tuning},
  journal   = {arXiv preprint arXiv:2012.13255},
  year      = {2020}
}

@inproceedings{austin2021structured,
  author    = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and {Van Den Berg}, Rian},
  title     = {Structured denoising diffusion models in discrete state-spaces},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {34},
  pages     = {17981--17993},
  year      = {2021}
}

@inproceedings{brown2020language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  title     = {Language models are few-shot learners},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {33},
  pages     = {1877--1901},
  year      = {2020}
}

@article{dettmers2023qlora,
  author    = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  title     = {{QLoRA}: Efficient finetuning of quantized {LLMs}},
  journal   = {arXiv preprint arXiv:2305.14314},
  year      = {2023}
}

@article{fedus2022switch,
  author    = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  title     = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  journal   = {Journal of Machine Learning Research},
  volume    = {23},
  number    = {120},
  pages     = {1--39},
  year      = {2022}
}

@inproceedings{hoogeboom2021autoregressive,
  author    = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  title     = {Argmax flows and multinomial diffusion: Learning categorical distributions},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {34},
  pages     = {12454--12465},
  year      = {2021}
}

@inproceedings{houlsby2019parameter,
  author    = {Houlsby, Neil and Giurgiu, Andrei and Jastrz{\k{e}}bski, Stanis{\l}aw and Morrone, Bruna and {De Laroussilhe}, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  title     = {Parameter-efficient transfer learning for {NLP}},
  booktitle = {International Conference on Machine Learning},
  pages     = {2790--2799},
  year      = {2019}
}

@article{hu2021lora,
  author    = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title     = {{LoRA}: Low-rank adaptation of large language models},
  journal   = {arXiv preprint arXiv:2106.09685},
  year      = {2021}
}

@article{ilharco2022editing,
  author    = {Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  title     = {Editing models with task arithmetic},
  journal   = {arXiv preprint arXiv:2212.04089},
  year      = {2022}
}

@inproceedings{lester2021power,
  author    = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  title     = {The power of scale for parameter-efficient prompt tuning},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages     = {3045--3059},
  year      = {2021}
}

@inproceedings{li2018measuring,
  author    = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  title     = {Measuring the intrinsic dimension of objective landscapes},
  booktitle = {International Conference on Learning Representations},
  year      = {2018}
}

@inproceedings{li2021prefix,
  author    = {Li, Xiang Lisa and Liang, Percy},
  title     = {Prefix-tuning: Optimizing continuous prompts for generation},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages     = {4582--4597},
  year      = {2021}
}

@inproceedings{li2022diffusion,
  author    = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S. and Hashimoto, Tatsunori B.},
  title     = {Diffusion-{LM} improves controllable text generation},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {4328--4343},
  year      = {2022}
}

@inproceedings{lou2023discrete,
  author    = {Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
  title     = {Discrete diffusion modeling by estimating the ratios of the data distribution},
  booktitle = {International Conference on Machine Learning},
  pages     = {22481--22505},
  year      = {2023}
}

@article{sahoo2024masked,
  author    = {Sahoo, Prateek and Nguyen, Hieu and Loh, Chien-Yu and Kumar, Ashish and Narasimhan, Karthik},
  title     = {Simple and effective masked diffusion language models},
  journal   = {arXiv preprint arXiv:2406.07524},
  year      = {2024}
}

@article{tishby2000information,
  author    = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  title     = {The information bottleneck method},
  journal   = {arXiv preprint physics/0004057},
  year      = {2000}
}

@inproceedings{tishby2015deep,
  author    = {Tishby, Naftali and Zaslavsky, Noga},
  title     = {Deep learning and the information bottleneck principle},
  booktitle = {IEEE Information Theory Workshop},
  pages     = {1--5},
  year      = {2015}
}

@article{wang2020orthogonal,
  author    = {Wang, Zifan and Zhang, Zichen and Lee, Chen-Yu and Zhang, Han and Sun, Ruixin and Ren, Xiang and Su, Guande and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  title     = {Learning to prompt for continual learning},
  journal   = {arXiv preprint arXiv:2112.08654},
  year      = {2020}
}

@inproceedings{wang2022super,
  author    = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amir and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  title     = {Super-{NaturalInstructions}: Generalization via declarative instructions on 1600+ {NLP} tasks},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages     = {5085--5109},
  year      = {2022}
}

@article{zaken2021bitfit,
  author    = {Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  title     = {{BitFit}: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  journal   = {arXiv preprint arXiv:2106.10199},
  year      = {2021}
}

@inproceedings{zhang2023adalora,
  author    = {Zhang, Qian and Chen, Muxin and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  title     = {{AdaLoRA}: Adaptive budget allocation for parameter-efficient fine-tuning},
  booktitle = {International Conference on Learning Representations},
  year      = {2023}
}

@article{tlora2024,
  author    = {{T-LoRA authors}},
  title     = {{T-LoRA}: Timestep-dependent low-rank adaptation for diffusion models},
  journal   = {arXiv preprint arXiv:XXXX.XXXXX},
  year      = {2024},
  note      = {Placeholder: Add actual citation details}
}

@article{foura2024,
  author    = {{FouRA authors}},
  title     = {{FouRA}: Frequency-domain low-rank adaptation with adaptive rank gating},
  journal   = {arXiv preprint arXiv:2406.08798},
  year      = {2024},
  note      = {Placeholder: Verify arXiv number}
}

@article{selora2024,
  author    = {{SeLoRA authors}},
  title     = {{SeLoRA}: Fisher-informed adaptive rank allocation for parameter-efficient fine-tuning},
  journal   = {arXiv preprint arXiv:2408.07196},
  year      = {2024},
  note      = {Placeholder: Verify arXiv number}
}

@article{gelora2024,
  author    = {{GeLoRA authors}},
  title     = {{GeLoRA}: Principled rank allocation based on intrinsic dimension},
  journal   = {arXiv preprint arXiv:2412.09250},
  year      = {2024},
  note      = {Placeholder: Verify arXiv number}
}

@article{estlora2024,
  author    = {{EST-LoRA authors}},
  title     = {{EST-LoRA}: Training-free adapter fusion via routing at inference},
  journal   = {arXiv preprint arXiv:2508.02165},
  year      = {2024},
  note      = {Placeholder: Verify arXiv number}
}
