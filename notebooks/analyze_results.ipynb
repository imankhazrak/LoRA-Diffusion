{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA-Diffusion Results Analysis\n",
    "\n",
    "This notebook provides analysis tools for LoRA-Diffusion experiments.\n",
    "\n",
    "**Contents:**\n",
    "1. Load and visualize training history\n",
    "2. Compare methods across tasks\n",
    "3. Analyze parameter efficiency\n",
    "4. Visualize step-adaptive ranks\n",
    "5. Performance vs. efficiency tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_history(output_dir):\n",
    "    \"\"\"Load training history from output directory.\"\"\"\n",
    "    history_file = Path(output_dir) / \"training_history.json\"\n",
    "    \n",
    "    if not history_file.exists():\n",
    "        print(f\"Warning: {history_file} not found\")\n",
    "        return None\n",
    "    \n",
    "    with open(history_file, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    return pd.DataFrame(history)\n",
    "\n",
    "# Example: Load a specific experiment\n",
    "output_dir = \"./outputs/sst2_lora_diffusion\"  # Change this to your output directory\n",
    "\n",
    "df = load_training_history(output_dir)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"Loaded {len(df)} training steps\")\n",
    "    print(\"\\nColumns:\", df.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No training history found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(df, metrics=['loss', 'accuracy']):\n",
    "    \"\"\"Plot training curves.\"\"\"\n",
    "    if df is None:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(metrics), figsize=(6*len(metrics), 5))\n",
    "    \n",
    "    if len(metrics) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, metric in zip(axes, metrics):\n",
    "        # Extract metric values\n",
    "        if 'metrics' in df.columns:\n",
    "            values = df['metrics'].apply(lambda x: x.get(metric, np.nan))\n",
    "        else:\n",
    "            values = df.get(metric, [])\n",
    "        \n",
    "        ax.plot(df['step'], values, linewidth=2)\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f'Training {metric.replace(\"_\", \" \").title()}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training curves\n",
    "if df is not None:\n",
    "    plot_training_curves(df, metrics=['loss', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Multiple Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiple_experiments(base_dir, experiment_names):\n",
    "    \"\"\"Load multiple experiment results.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name in experiment_names:\n",
    "        exp_dir = Path(base_dir) / name\n",
    "        df = load_training_history(exp_dir)\n",
    "        if df is not None:\n",
    "            results[name] = df\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Compare different methods on SST-2\n",
    "experiment_names = [\n",
    "    \"sst2_lora_diffusion\",\n",
    "    \"sst2_weight_lora\",\n",
    "    \"sst2_full_ft\",\n",
    "    \"sst2_adapters\",\n",
    "]\n",
    "\n",
    "base_dir = \"./outputs\"\n",
    "experiments = load_multiple_experiments(base_dir, experiment_names)\n",
    "\n",
    "print(f\"Loaded {len(experiments)} experiments\")\n",
    "for name in experiments:\n",
    "    print(f\"  - {name}: {len(experiments[name])} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_method_comparison(experiments, metric='loss'):\n",
    "    \"\"\"Compare multiple methods.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for name, df in experiments.items():\n",
    "        if 'metrics' in df.columns:\n",
    "            values = df['metrics'].apply(lambda x: x.get(metric, np.nan))\n",
    "        else:\n",
    "            values = df.get(metric, [])\n",
    "        \n",
    "        # Clean name for legend\n",
    "        method_name = name.split('_', 1)[1].replace('_', ' ').title()\n",
    "        plt.plot(df['step'], values, label=method_name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.title(f'Method Comparison: {metric.replace(\"_\", \" \").title()}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot comparison\n",
    "if experiments:\n",
    "    plot_method_comparison(experiments, metric='loss')\n",
    "    plot_method_comparison(experiments, metric='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_parameter_efficiency(checkpoint_dirs):\n",
    "    \"\"\"Analyze parameter efficiency of different methods.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, checkpoint_dir in checkpoint_dirs.items():\n",
    "        config_file = Path(checkpoint_dir) / \"config.json\"\n",
    "        \n",
    "        if not config_file.exists():\n",
    "            continue\n",
    "        \n",
    "        with open(config_file, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Calculate parameter counts (simplified)\n",
    "        # In practice, you'd load the actual model\n",
    "        method = config.get('method', {}).get('name', 'unknown')\n",
    "        \n",
    "        # Approximate parameter counts\n",
    "        param_counts = {\n",
    "            'lora_diffusion': 0.7,\n",
    "            'weight_lora': 0.9,\n",
    "            'adapters': 2.1,\n",
    "            'prefix_tuning': 1.0,\n",
    "            'bitfit': 0.1,\n",
    "            'full_ft': 100.0,\n",
    "        }\n",
    "        \n",
    "        results.append({\n",
    "            'Method': name.split('_', 1)[1].replace('_', ' ').title(),\n",
    "            'Trainable %': param_counts.get(method, 1.0),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example parameter efficiency data\n",
    "param_data = pd.DataFrame([\n",
    "    {'Method': 'LoRA-Diffusion', 'Trainable %': 0.7, 'Performance': 98.1},\n",
    "    {'Method': 'Weight LoRA', 'Trainable %': 0.9, 'Performance': 94.1},\n",
    "    {'Method': 'Adapters', 'Trainable %': 2.1, 'Performance': 96.1},\n",
    "    {'Method': 'Prefix Tuning', 'Trainable %': 1.0, 'Performance': 92.1},\n",
    "    {'Method': 'BitFit', 'Trainable %': 0.1, 'Performance': 86.5},\n",
    "    {'Method': 'Full FT', 'Trainable %': 100.0, 'Performance': 100.0},\n",
    "])\n",
    "\n",
    "display(param_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_efficiency_frontier(df):\n",
    "    \"\"\"Plot performance vs. parameter efficiency.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        df['Trainable %'],\n",
    "        df['Performance'],\n",
    "        s=200,\n",
    "        alpha=0.6,\n",
    "        c=range(len(df)),\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    for _, row in df.iterrows():\n",
    "        ax.annotate(\n",
    "            row['Method'],\n",
    "            (row['Trainable %'], row['Performance']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=10,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7)\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Trainable Parameters (%)', fontsize=12)\n",
    "    ax.set_ylabel('Performance (% of Full FT)', fontsize=12)\n",
    "    ax.set_title('Parameter Efficiency Frontier', fontsize=14, fontweight='bold')\n",
    "    ax.set_xscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_efficiency_frontier(param_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step-Adaptive Rank Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rank_schedule(num_steps=100):\n",
    "    \"\"\"Visualize step-adaptive rank allocation.\"\"\"\n",
    "    # Define rank schedule\n",
    "    steps = np.arange(num_steps)\n",
    "    ranks = np.zeros(num_steps)\n",
    "    scalings = np.zeros(num_steps)\n",
    "    \n",
    "    # Apply schedule\n",
    "    for t in range(num_steps):\n",
    "        if t > 2 * num_steps // 3:  # Early\n",
    "            ranks[t] = 64\n",
    "            scalings[t] = 1.0\n",
    "        elif t > num_steps // 3:  # Mid\n",
    "            ranks[t] = 32\n",
    "            scalings[t] = 0.5\n",
    "        else:  # Late\n",
    "            ranks[t] = 8\n",
    "            scalings[t] = 0.25\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "    \n",
    "    # Rank\n",
    "    ax1.plot(steps, ranks, linewidth=3, color='steelblue')\n",
    "    ax1.set_ylabel('Rank (r)', fontsize=12)\n",
    "    ax1.set_title('Step-Adaptive Rank Allocation', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 70])\n",
    "    \n",
    "    # Add phase labels\n",
    "    ax1.axvspan(67, 100, alpha=0.2, color='red', label='Early (r=64)')\n",
    "    ax1.axvspan(34, 67, alpha=0.2, color='orange', label='Mid (r=32)')\n",
    "    ax1.axvspan(0, 34, alpha=0.2, color='green', label='Late (r=8)')\n",
    "    ax1.legend(loc='upper right')\n",
    "    \n",
    "    # Scaling\n",
    "    ax2.plot(steps, scalings, linewidth=3, color='coral')\n",
    "    ax2.set_xlabel('Diffusion Step (t)', fontsize=12)\n",
    "    ax2.set_ylabel('Scaling (Ïƒ)', fontsize=12)\n",
    "    ax2.set_title('Step-Adaptive Scaling', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim([0, 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_rank_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Compare Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_results(result_files):\n",
    "    \"\"\"Load evaluation results from multiple experiments.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, file_path in result_files.items():\n",
    "        file_path = Path(file_path)\n",
    "        if not file_path.exists():\n",
    "            continue\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        metrics = data.get('metrics', {})\n",
    "        metrics['Method'] = name\n",
    "        results.append(metrics)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Load results (update paths as needed)\n",
    "result_files = {\n",
    "    'LoRA-Diffusion': './outputs/sst2_lora_diffusion/eval_results.json',\n",
    "    'Weight LoRA': './outputs/sst2_weight_lora/eval_results.json',\n",
    "    'Full FT': './outputs/sst2_full_ft/eval_results.json',\n",
    "}\n",
    "\n",
    "# results_df = load_evaluation_results(result_files)\n",
    "# display(results_df)\n",
    "\n",
    "print(\"Note: Update result_files paths with your actual output directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Task Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_heatmap(data):\n",
    "    \"\"\"Create heatmap comparing methods across tasks.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        data,\n",
    "        annot=True,\n",
    "        fmt='.1f',\n",
    "        cmap='RdYlGn',\n",
    "        center=80,\n",
    "        vmin=70,\n",
    "        vmax=95,\n",
    "        cbar_kws={'label': 'Performance Score'}\n",
    "    )\n",
    "    \n",
    "    plt.title('Method Performance Across Tasks', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Task', fontsize=12)\n",
    "    plt.ylabel('Method', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example data (from paper Table 3)\n",
    "comparison_data = pd.DataFrame({\n",
    "    'SST-2': [94.2, 92.1, 93.4, 91.8, 94.5],\n",
    "    'SQuAD': [87.9, 84.3, 86.9, 83.1, 88.7],\n",
    "    'XSum': [37.4, 35.2, 36.9, 34.3, 37.8],\n",
    "}, index=['LoRA-Diffusion', 'Weight LoRA', 'Adapters', 'Prefix', 'Full FT'])\n",
    "\n",
    "create_comparison_heatmap(comparison_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_time_comparison(data):\n",
    "    \"\"\"Compare training times.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.9, len(data)))\n",
    "    bars = ax.barh(data['Method'], data['Time (hours)'], color=colors)\n",
    "    \n",
    "    ax.set_xlabel('Training Time (hours)', fontsize=12)\n",
    "    ax.set_title('Training Time Comparison (SST-2, 5000 steps)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, time) in enumerate(zip(bars, data['Time (hours)'])):\n",
    "        ax.text(time + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                f'{time:.2f}h', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example data\n",
    "time_data = pd.DataFrame([\n",
    "    {'Method': 'LoRA-Diffusion', 'Time (hours)': 0.45},\n",
    "    {'Method': 'Weight LoRA', 'Time (hours)': 0.54},\n",
    "    {'Method': 'Adapters', 'Time (hours)': 0.72},\n",
    "    {'Method': 'Full FT', 'Time (hours)': 0.91},\n",
    "])\n",
    "\n",
    "plot_training_time_comparison(time_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_statistics(experiments):\n",
    "    \"\"\"Print summary statistics for experiments.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for name, df in experiments.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get final metrics\n",
    "        if 'metrics' in df.columns and len(df) > 0:\n",
    "            final_metrics = df.iloc[-1]['metrics']\n",
    "            print(f\"  Final Loss: {final_metrics.get('loss', 'N/A'):.4f}\")\n",
    "            print(f\"  Final Accuracy: {final_metrics.get('accuracy', 'N/A'):.4f}\")\n",
    "        \n",
    "        # Training time\n",
    "        if 'time_per_step' in df.columns:\n",
    "            total_time = df['time_per_step'].sum() / 3600  # Convert to hours\n",
    "            print(f\"  Total Training Time: {total_time:.2f} hours\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "if experiments:\n",
    "    print_summary_statistics(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_latex_table(df, caption=\"Results comparison\", label=\"tab:results\"):\n",
    "    \"\"\"Export results as LaTeX table.\"\"\"\n",
    "    latex = df.to_latex(\n",
    "        index=True,\n",
    "        float_format=\"%.2f\",\n",
    "        caption=caption,\n",
    "        label=label,\n",
    "        escape=False,\n",
    "    )\n",
    "    \n",
    "    print(\"LaTeX Table:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(latex)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Save to file\n",
    "    with open('results_table.tex', 'w') as f:\n",
    "        f.write(latex)\n",
    "    print(\"\\nSaved to results_table.tex\")\n",
    "\n",
    "# Example\n",
    "# export_latex_table(comparison_data, caption=\"Performance across tasks\", label=\"tab:performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides tools for analyzing LoRA-Diffusion results. Key analyses:\n",
    "\n",
    "1. **Training curves**: Monitor loss and accuracy over time\n",
    "2. **Method comparison**: Compare different PEFT methods\n",
    "3. **Parameter efficiency**: Analyze trainable parameters vs. performance\n",
    "4. **Step-adaptive ranks**: Visualize rank allocation strategy\n",
    "5. **Multi-task results**: Compare performance across tasks\n",
    "6. **Training time**: Analyze computational efficiency\n",
    "\n",
    "**Next steps:**\n",
    "- Update paths to your actual experiment directories\n",
    "- Run your own analyses\n",
    "- Export results for papers/presentations\n",
    "- Customize visualizations as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
