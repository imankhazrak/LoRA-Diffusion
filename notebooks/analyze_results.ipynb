{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA-Diffusion Results Analysis\n",
        "\n",
        "This notebook provides analysis tools for LoRA-Diffusion experiments.\n",
        "\n",
        "**Contents:**\n",
        "1. Load and visualize training history\n",
        "2. Compare methods across tasks\n",
        "3. Analyze parameter efficiency\n",
        "4. Visualize step-adaptive ranks\n",
        "5. Performance vs. efficiency tradeoffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"Imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_training_history(output_dir):\n",
        "    \"\"\"Load training history from output directory.\"\"\"\n",
        "    history_file = Path(output_dir) / \"training_history.json\"\n",
        "    \n",
        "    if not history_file.exists():\n",
        "        print(f\"Warning: {history_file} not found\")\n",
        "        return None\n",
        "    \n",
        "    with open(history_file, 'r') as f:\n",
        "        history = json.load(f)\n",
        "    \n",
        "    return pd.DataFrame(history)\n",
        "\n",
        "# Example: Load a specific experiment\n",
        "output_dir = \"./outputs/sst2_lora_diffusion\"  # Change this to your output directory\n",
        "\n",
        "df = load_training_history(output_dir)\n",
        "\n",
        "if df is not None:\n",
        "    print(f\"Loaded {len(df)} training steps\")\n",
        "    print(\"\\nColumns:\", df.columns.tolist())\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"No training history found. Run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Visualize Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_curves(df, metrics=['loss', 'accuracy']):\n",
        "    \"\"\"Plot training curves.\"\"\"\n",
        "    if df is None:\n",
        "        print(\"No data to plot\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(1, len(metrics), figsize=(6*len(metrics), 5))\n",
        "    \n",
        "    if len(metrics) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for ax, metric in zip(axes, metrics):\n",
        "        # Extract metric values\n",
        "        if 'metrics' in df.columns:\n",
        "            values = df['metrics'].apply(lambda x: x.get(metric, np.nan))\n",
        "        else:\n",
        "            values = df.get(metric, [])\n",
        "        \n",
        "        ax.plot(df['step'], values, linewidth=2)\n",
        "        ax.set_xlabel('Training Step')\n",
        "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
        "        ax.set_title(f'Training {metric.replace(\"_\", \" \").title()}')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot training curves\n",
        "if df is not None:\n",
        "    plot_training_curves(df, metrics=['loss', 'accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compare Multiple Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_multiple_experiments(base_dir, experiment_names):\n",
        "    \"\"\"Load multiple experiment results.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for name in experiment_names:\n",
        "        exp_dir = Path(base_dir) / name\n",
        "        df = load_training_history(exp_dir)\n",
        "        if df is not None:\n",
        "            results[name] = df\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example: Compare different methods on SST-2\n",
        "experiment_names = [\n",
        "    \"sst2_lora_diffusion\",\n",
        "    \"sst2_weight_lora\",\n",
        "    \"sst2_full_ft\",\n",
        "    \"sst2_adapters\",\n",
        "]\n",
        "\n",
        "base_dir = \"./outputs\"\n",
        "experiments = load_multiple_experiments(base_dir, experiment_names)\n",
        "\n",
        "print(f\"Loaded {len(experiments)} experiments\")\n",
        "for name in experiments:\n",
        "    print(f\"  - {name}: {len(experiments[name])} steps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_method_comparison(experiments, metric='loss'):\n",
        "    \"\"\"Compare multiple methods.\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    for name, df in experiments.items():\n",
        "        if 'metrics' in df.columns:\n",
        "            values = df['metrics'].apply(lambda x: x.get(metric, np.nan))\n",
        "        else:\n",
        "            values = df.get(metric, [])\n",
        "        \n",
        "        # Clean name for legend\n",
        "        method_name = name.split('_', 1)[1].replace('_', ' ').title()\n",
        "        plt.plot(df['step'], values, label=method_name, linewidth=2)\n",
        "    \n",
        "    plt.xlabel('Training Step')\n",
        "    plt.ylabel(metric.replace('_', ' ').title())\n",
        "    plt.title(f'Method Comparison: {metric.replace(\"_\", \" \").title()}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot comparison\n",
        "if experiments:\n",
        "    plot_method_comparison(experiments, metric='loss')\n",
        "    plot_method_comparison(experiments, metric='accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Parameter Efficiency Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_parameter_efficiency(checkpoint_dirs):\n",
        "    \"\"\"Analyze parameter efficiency of different methods.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for name, checkpoint_dir in checkpoint_dirs.items():\n",
        "        config_file = Path(checkpoint_dir) / \"config.json\"\n",
        "        \n",
        "        if not config_file.exists():\n",
        "            continue\n",
        "        \n",
        "        with open(config_file, 'r') as f:\n",
        "            config = json.load(f)\n",
        "        \n",
        "        # Calculate parameter counts (simplified)\n",
        "        # In practice, you'd load the actual model\n",
        "        method = config.get('method', {}).get('name', 'unknown')\n",
        "        \n",
        "        # Approximate parameter counts\n",
        "        param_counts = {\n",
        "            'lora_diffusion': 0.7,\n",
        "            'weight_lora': 0.9,\n",
        "            'adapters': 2.1,\n",
        "            'prefix_tuning': 1.0,\n",
        "            'bitfit': 0.1,\n",
        "            'full_ft': 100.0,\n",
        "        }\n",
        "        \n",
        "        results.append({\n",
        "            'Method': name.split('_', 1)[1].replace('_', ' ').title(),\n",
        "            'Trainable %': param_counts.get(method, 1.0),\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example parameter efficiency data\n",
        "param_data = pd.DataFrame([\n",
        "    {'Method': 'LoRA-Diffusion', 'Trainable %': 0.7, 'Performance': 98.1},\n",
        "    {'Method': 'Weight LoRA', 'Trainable %': 0.9, 'Performance': 94.1},\n",
        "    {'Method': 'Adapters', 'Trainable %': 2.1, 'Performance': 96.1},\n",
        "    {'Method': 'Prefix Tuning', 'Trainable %': 1.0, 'Performance': 92.1},\n",
        "    {'Method': 'BitFit', 'Trainable %': 0.1, 'Performance': 86.5},\n",
        "    {'Method': 'Full FT', 'Trainable %': 100.0, 'Performance': 100.0},\n",
        "])\n",
        "\n",
        "display(param_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_efficiency_frontier(df):\n",
        "    \"\"\"Plot performance vs. parameter efficiency.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    # Scatter plot\n",
        "    scatter = ax.scatter(\n",
        "        df['Trainable %'],\n",
        "        df['Performance'],\n",
        "        s=200,\n",
        "        alpha=0.6,\n",
        "        c=range(len(df)),\n",
        "        cmap='viridis'\n",
        "    )\n",
        "    \n",
        "    # Add labels\n",
        "    for _, row in df.iterrows():\n",
        "        ax.annotate(\n",
        "            row['Method'],\n",
        "            (row['Trainable %'], row['Performance']),\n",
        "            xytext=(5, 5),\n",
        "            textcoords='offset points',\n",
        "            fontsize=10,\n",
        "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7)\n",
        "        )\n",
        "    \n",
        "    ax.set_xlabel('Trainable Parameters (%)', fontsize=12)\n",
        "    ax.set_ylabel('Performance (% of Full FT)', fontsize=12)\n",
        "    ax.set_title('Parameter Efficiency Frontier', fontsize=14, fontweight='bold')\n",
        "    ax.set_xscale('log')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_efficiency_frontier(param_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Step-Adaptive Rank Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_rank_schedule(num_steps=100):\n",
        "    \"\"\"Visualize step-adaptive rank allocation.\"\"\"\n",
        "    # Define rank schedule\n",
        "    steps = np.arange(num_steps)\n",
        "    ranks = np.zeros(num_steps)\n",
        "    scalings = np.zeros(num_steps)\n",
        "    \n",
        "    # Apply schedule\n",
        "    for t in range(num_steps):\n",
        "        if t > 2 * num_steps // 3:  # Early\n",
        "            ranks[t] = 64\n",
        "            scalings[t] = 1.0\n",
        "        elif t > num_steps // 3:  # Mid\n",
        "            ranks[t] = 32\n",
        "            scalings[t] = 0.5\n",
        "        else:  # Late\n",
        "            ranks[t] = 8\n",
        "            scalings[t] = 0.25\n",
        "    \n",
        "    # Plot\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
        "    \n",
        "    # Rank\n",
        "    ax1.plot(steps, ranks, linewidth=3, color='steelblue')\n",
        "    ax1.set_ylabel('Rank (r)', fontsize=12)\n",
        "    ax1.set_title('Step-Adaptive Rank Allocation', fontsize=14, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_ylim([0, 70])\n",
        "    \n",
        "    # Add phase labels\n",
        "    ax1.axvspan(67, 100, alpha=0.2, color='red', label='Early (r=64)')\n",
        "    ax1.axvspan(34, 67, alpha=0.2, color='orange', label='Mid (r=32)')\n",
        "    ax1.axvspan(0, 34, alpha=0.2, color='green', label='Late (r=8)')\n",
        "    ax1.legend(loc='upper right')\n",
        "    \n",
        "    # Scaling\n",
        "    ax2.plot(steps, scalings, linewidth=3, color='coral')\n",
        "    ax2.set_xlabel('Diffusion Step (t)', fontsize=12)\n",
        "    ax2.set_ylabel('Scaling (σ)', fontsize=12)\n",
        "    ax2.set_title('Step-Adaptive Scaling', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim([0, 1.1])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_rank_schedule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load and Compare Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_evaluation_results(result_files):\n",
        "    \"\"\"Load evaluation results from multiple experiments.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for name, file_path in result_files.items():\n",
        "        file_path = Path(file_path)\n",
        "        if not file_path.exists():\n",
        "            continue\n",
        "        \n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        metrics = data.get('metrics', {})\n",
        "        metrics['Method'] = name\n",
        "        results.append(metrics)\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example: Load results (update paths as needed)\n",
        "result_files = {\n",
        "    'LoRA-Diffusion': './outputs/sst2_lora_diffusion/eval_results.json',\n",
        "    'Weight LoRA': './outputs/sst2_weight_lora/eval_results.json',\n",
        "    'Full FT': './outputs/sst2_full_ft/eval_results.json',\n",
        "}\n",
        "\n",
        "# results_df = load_evaluation_results(result_files)\n",
        "# display(results_df)\n",
        "\n",
        "print(\"Note: Update result_files paths with your actual output directories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Multi-Task Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_heatmap(data):\n",
        "    \"\"\"Create heatmap comparing methods across tasks.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    sns.heatmap(\n",
        "        data,\n",
        "        annot=True,\n",
        "        fmt='.1f',\n",
        "        cmap='RdYlGn',\n",
        "        center=80,\n",
        "        vmin=70,\n",
        "        vmax=95,\n",
        "        cbar_kws={'label': 'Performance Score'}\n",
        "    )\n",
        "    \n",
        "    plt.title('Method Performance Across Tasks', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Task', fontsize=12)\n",
        "    plt.ylabel('Method', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example data (from paper Table 3)\n",
        "comparison_data = pd.DataFrame({\n",
        "    'SST-2': [94.2, 92.1, 93.4, 91.8, 94.5],\n",
        "    'SQuAD': [87.9, 84.3, 86.9, 83.1, 88.7],\n",
        "    'XSum': [37.4, 35.2, 36.9, 34.3, 37.8],\n",
        "}, index=['LoRA-Diffusion', 'Weight LoRA', 'Adapters', 'Prefix', 'Full FT'])\n",
        "\n",
        "create_comparison_heatmap(comparison_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Time Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_time_comparison(data):\n",
        "    \"\"\"Compare training times.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    colors = plt.cm.viridis(np.linspace(0, 0.9, len(data)))\n",
        "    bars = ax.barh(data['Method'], data['Time (hours)'], color=colors)\n",
        "    \n",
        "    ax.set_xlabel('Training Time (hours)', fontsize=12)\n",
        "    ax.set_title('Training Time Comparison (SST-2, 5000 steps)', fontsize=14, fontweight='bold')\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, (bar, time) in enumerate(zip(bars, data['Time (hours)'])):\n",
        "        ax.text(time + 0.02, bar.get_y() + bar.get_height()/2, \n",
        "                f'{time:.2f}h', va='center', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example data\n",
        "time_data = pd.DataFrame([\n",
        "    {'Method': 'LoRA-Diffusion', 'Time (hours)': 0.45},\n",
        "    {'Method': 'Weight LoRA', 'Time (hours)': 0.54},\n",
        "    {'Method': 'Adapters', 'Time (hours)': 0.72},\n",
        "    {'Method': 'Full FT', 'Time (hours)': 0.91},\n",
        "])\n",
        "\n",
        "plot_training_time_comparison(time_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_summary_statistics(experiments):\n",
        "    \"\"\"Print summary statistics for experiments.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"EXPERIMENT SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for name, df in experiments.items():\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        # Get final metrics\n",
        "        if 'metrics' in df.columns and len(df) > 0:\n",
        "            final_metrics = df.iloc[-1]['metrics']\n",
        "            print(f\"  Final Loss: {final_metrics.get('loss', 'N/A'):.4f}\")\n",
        "            print(f\"  Final Accuracy: {final_metrics.get('accuracy', 'N/A'):.4f}\")\n",
        "        \n",
        "        # Training time\n",
        "        if 'time_per_step' in df.columns:\n",
        "            total_time = df['time_per_step'].sum() / 3600  # Convert to hours\n",
        "            print(f\"  Total Training Time: {total_time:.2f} hours\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "if experiments:\n",
        "    print_summary_statistics(experiments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export Results for Paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_latex_table(df, caption=\"Results comparison\", label=\"tab:results\"):\n",
        "    \"\"\"Export results as LaTeX table.\"\"\"\n",
        "    latex = df.to_latex(\n",
        "        index=True,\n",
        "        float_format=\"%.2f\",\n",
        "        caption=caption,\n",
        "        label=label,\n",
        "        escape=False,\n",
        "    )\n",
        "    \n",
        "    print(\"LaTeX Table:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(latex)\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Save to file\n",
        "    with open('results_table.tex', 'w') as f:\n",
        "        f.write(latex)\n",
        "    print(\"\\nSaved to results_table.tex\")\n",
        "\n",
        "# Example\n",
        "# export_latex_table(comparison_data, caption=\"Performance across tasks\", label=\"tab:performance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Generate Paper Figures\n",
        "\n",
        "Generate the required figures for the paper as specified in `doc/figures/README.md`:\n",
        "1. Rank ablation study\n",
        "2. Effective rank across diffusion steps\n",
        "3. Data efficiency comparison\n",
        "4. Trajectory visualization (t-SNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup output directory for figures\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "figures_dir = Path(\"../doc/figures\")\n",
        "figures_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Figures will be saved to: {figures_dir.absolute()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 1: Rank Ablation Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rank_ablation():\n",
        "    \"\"\"Generate rank ablation figure: Rank vs. performance (left) and vs. trainable parameters (right).\"\"\"\n",
        "    \n",
        "    # Example data - replace with actual experimental results\n",
        "    # Rank configurations: fixed ranks vs. step-adaptive (8/32/64)\n",
        "    ranks = [4, 8, 16, 32, 64, 128, '8/32/64 (adaptive)']\n",
        "    performance = [88.5, 91.2, 93.8, 95.1, 95.8, 96.2, 96.5]  # Performance scores\n",
        "    trainable_params = [0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 1.1]  # Trainable params in millions\n",
        "    \n",
        "    # Convert adaptive rank to numeric for plotting\n",
        "    rank_numeric = [4, 8, 16, 32, 64, 128, 35]  # Approximate for adaptive\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # Left plot: Rank vs. Performance\n",
        "    colors = ['steelblue'] * 6 + ['red']\n",
        "    markers = ['o'] * 6 + ['*']\n",
        "    sizes = [100] * 6 + [200]\n",
        "    \n",
        "    for i, (r, r_num, perf, c, m, s) in enumerate(zip(ranks, rank_numeric, performance, colors, markers, sizes)):\n",
        "        if isinstance(r, str):\n",
        "            ax1.scatter(r_num, perf, c=c, marker=m, s=s, label=r, zorder=3, edgecolors='black', linewidths=2)\n",
        "        else:\n",
        "            ax1.scatter(r_num, perf, c=c, marker=m, s=s, zorder=2, edgecolors='black', linewidths=1)\n",
        "    \n",
        "    ax1.plot(rank_numeric[:6], performance[:6], '--', alpha=0.3, color='gray', zorder=1)\n",
        "    ax1.set_xlabel('Rank (r)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Performance (%)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_title('Rank vs. Performance', fontsize=16, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend(loc='lower right', fontsize=11)\n",
        "    ax1.set_xscale('log', base=2)\n",
        "    ax1.set_xticks([4, 8, 16, 32, 64, 128])\n",
        "    ax1.set_xticklabels(['4', '8', '16', '32', '64', '128'])\n",
        "    \n",
        "    # Right plot: Rank vs. Trainable Parameters\n",
        "    for i, (r, r_num, params, c, m, s) in enumerate(zip(ranks, rank_numeric, trainable_params, colors, markers, sizes)):\n",
        "        if isinstance(r, str):\n",
        "            ax2.scatter(r_num, params, c=c, marker=m, s=s, label=r, zorder=3, edgecolors='black', linewidths=2)\n",
        "        else:\n",
        "            ax2.scatter(r_num, params, c=c, marker=m, s=s, zorder=2, edgecolors='black', linewidths=1)\n",
        "    \n",
        "    ax2.plot(rank_numeric[:6], trainable_params[:6], '--', alpha=0.3, color='gray', zorder=1)\n",
        "    ax2.set_xlabel('Rank (r)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Trainable Parameters (M)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_title('Rank vs. Trainable Parameters', fontsize=16, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend(loc='upper left', fontsize=11)\n",
        "    ax2.set_xscale('log', base=2)\n",
        "    ax2.set_xticks([4, 8, 16, 32, 64, 128])\n",
        "    ax2.set_xticklabels(['4', '8', '16', '32', '64', '128'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save as PNG and PDF\n",
        "    output_path_png = figures_dir / \"rank_ablation.png\"\n",
        "    output_path_pdf = figures_dir / \"rank_ablation.pdf\"\n",
        "    plt.savefig(output_path_png, dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(output_path_pdf, bbox_inches='tight')\n",
        "    print(f\"Saved: {output_path_png}\")\n",
        "    print(f\"Saved: {output_path_pdf}\")\n",
        "    plt.show()\n",
        "\n",
        "plot_rank_ablation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2: Effective Rank Across Diffusion Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_effective_rank():\n",
        "    \"\"\"Generate effective rank figure: Effective rank of LoRA modules across diffusion steps.\"\"\"\n",
        "    \n",
        "    # Diffusion steps (t=0 is final step, t=100 is initial noise)\n",
        "    num_steps = 100\n",
        "    steps = np.arange(num_steps)\n",
        "    \n",
        "    # Effective rank decreases as we go from early (high noise) to late (low noise) steps\n",
        "    # Early steps (high t) have higher effective rank, late steps (low t) have lower effective rank\n",
        "    effective_rank = np.zeros(num_steps)\n",
        "    \n",
        "    # Simulate effective rank pattern: higher at early steps, lower at late steps\n",
        "    for t in range(num_steps):\n",
        "        if t > 2 * num_steps // 3:  # Early steps (t > 66)\n",
        "            # High effective rank, close to full rank\n",
        "            effective_rank[t] = 55 + 5 * np.sin(t * 0.1) + np.random.normal(0, 2)\n",
        "        elif t > num_steps // 3:  # Mid steps (33 < t <= 66)\n",
        "            # Medium effective rank\n",
        "            effective_rank[t] = 28 + 3 * np.sin(t * 0.15) + np.random.normal(0, 1.5)\n",
        "        else:  # Late steps (t <= 33)\n",
        "            # Lower effective rank\n",
        "            effective_rank[t] = 6 + 2 * np.sin(t * 0.2) + np.random.normal(0, 1)\n",
        "    \n",
        "    # Clip to reasonable bounds\n",
        "    effective_rank = np.clip(effective_rank, 0, 64)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    \n",
        "    # Plot effective rank\n",
        "    ax.plot(steps, effective_rank, linewidth=2.5, color='steelblue', alpha=0.8)\n",
        "    \n",
        "    # Add shaded regions for different phases\n",
        "    ax.axvspan(67, 100, alpha=0.15, color='red', label='Early Steps (High Effective Rank)')\n",
        "    ax.axvspan(34, 67, alpha=0.15, color='orange', label='Mid Steps (Medium Effective Rank)')\n",
        "    ax.axvspan(0, 34, alpha=0.15, color='green', label='Late Steps (Low Effective Rank)')\n",
        "    \n",
        "    # Add horizontal lines for rank thresholds\n",
        "    ax.axhline(y=64, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    ax.axhline(y=32, color='orange', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    ax.axhline(y=8, color='green', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    \n",
        "    ax.set_xlabel('Diffusion Step (t)', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Effective Rank', fontsize=14, fontweight='bold')\n",
        "    ax.set_title('Effective Rank of LoRA Modules Across Diffusion Steps', fontsize=16, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc='upper right', fontsize=11)\n",
        "    ax.set_xlim([0, 100])\n",
        "    ax.set_ylim([0, 65])\n",
        "    \n",
        "    # Invert x-axis to show progression from noise (right) to clean (left)\n",
        "    # This is more intuitive: early steps (high noise) on right, late steps (clean) on left\n",
        "    ax.invert_xaxis()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save as PNG and PDF\n",
        "    output_path_png = figures_dir / \"effective_rank.png\"\n",
        "    output_path_pdf = figures_dir / \"effective_rank.pdf\"\n",
        "    plt.savefig(output_path_png, dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(output_path_pdf, bbox_inches='tight')\n",
        "    print(f\"Saved: {output_path_png}\")\n",
        "    print(f\"Saved: {output_path_pdf}\")\n",
        "    plt.show()\n",
        "\n",
        "plot_effective_rank()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 3: Data Efficiency Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_data_efficiency():\n",
        "    \"\"\"Generate data efficiency figure: Performance vs. training data size.\"\"\"\n",
        "    \n",
        "    # Training data sizes (as percentage of full dataset)\n",
        "    data_sizes = np.array([10, 20, 30, 50, 70, 100])  # Percentage\n",
        "    \n",
        "    # Performance for LoRA-Diffusion (better data efficiency)\n",
        "    lora_diffusion_perf = np.array([75.2, 82.5, 87.3, 91.8, 94.2, 96.5])\n",
        "    \n",
        "    # Performance for Weight LoRA (lower data efficiency)\n",
        "    weight_lora_perf = np.array([65.1, 72.3, 78.9, 85.2, 89.1, 94.1])\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 7))\n",
        "    \n",
        "    # Plot both methods\n",
        "    ax.plot(data_sizes, lora_diffusion_perf, 'o-', linewidth=2.5, markersize=10, \n",
        "            label='LoRA-Diffusion', color='steelblue', zorder=3)\n",
        "    ax.plot(data_sizes, weight_lora_perf, 's--', linewidth=2.5, markersize=10, \n",
        "            label='Weight LoRA', color='coral', zorder=2, alpha=0.8)\n",
        "    \n",
        "    # Fill area between curves to highlight advantage\n",
        "    ax.fill_between(data_sizes, weight_lora_perf, lora_diffusion_perf, \n",
        "                     alpha=0.2, color='steelblue', label='LoRA-Diffusion Advantage')\n",
        "    \n",
        "    ax.set_xlabel('Training Data Size (% of Full Dataset)', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Performance (%)', fontsize=14, fontweight='bold')\n",
        "    ax.set_title('Data Efficiency: Performance vs. Training Data Size', fontsize=16, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend(loc='lower right', fontsize=12)\n",
        "    ax.set_xlim([5, 105])\n",
        "    ax.set_ylim([60, 100])\n",
        "    \n",
        "    # Add annotations for key points\n",
        "    ax.annotate('Better data efficiency', \n",
        "                xy=(30, 87), xytext=(25, 95),\n",
        "                arrowprops=dict(arrowstyle='->', color='steelblue', lw=2),\n",
        "                fontsize=11, color='steelblue', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save as PNG and PDF\n",
        "    output_path_png = figures_dir / \"data_efficiency.png\"\n",
        "    output_path_pdf = figures_dir / \"data_efficiency.pdf\"\n",
        "    plt.savefig(output_path_png, dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(output_path_pdf, bbox_inches='tight')\n",
        "    print(f\"Saved: {output_path_png}\")\n",
        "    print(f\"Saved: {output_path_pdf}\")\n",
        "    plt.show()\n",
        "\n",
        "plot_data_efficiency()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 4: Trajectory Visualization (t-SNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_trajectory_visualization():\n",
        "    \"\"\"Generate t-SNE visualization of denoising trajectories showing task-specific clusters.\"\"\"\n",
        "    \n",
        "    try:\n",
        "        from sklearn.manifold import TSNE\n",
        "        from sklearn.decomposition import PCA\n",
        "    except ImportError:\n",
        "        print(\"Warning: sklearn not available. Installing...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', 'scikit-learn'])\n",
        "        from sklearn.manifold import TSNE\n",
        "        from sklearn.decomposition import PCA\n",
        "    \n",
        "    # Simulate trajectory embeddings for different tasks\n",
        "    # In practice, these would come from actual model embeddings at different diffusion steps\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    n_samples_per_task = 50\n",
        "    n_steps_per_trajectory = 10  # Sample 10 steps per trajectory\n",
        "    \n",
        "    tasks = ['SST-2', 'SQuAD', 'XSum', 'AGNews']\n",
        "    colors = ['steelblue', 'coral', 'green', 'purple']\n",
        "    \n",
        "    # Generate synthetic trajectory data\n",
        "    # Each task has distinct clusters in embedding space\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "    all_tasks = []\n",
        "    \n",
        "    for task_idx, (task, color) in enumerate(zip(tasks, colors)):\n",
        "        # Create task-specific cluster center\n",
        "        center = np.random.randn(128) * 5  # 128-dim embedding\n",
        "        center[task_idx * 32:(task_idx + 1) * 32] += 10  # Make task-specific dimensions prominent\n",
        "        \n",
        "        for sample_idx in range(n_samples_per_task):\n",
        "            # Generate trajectory: start from noise, converge to task-specific region\n",
        "            for step in range(n_steps_per_trajectory):\n",
        "                # Progressively move from random noise to task center\n",
        "                noise_level = 1.0 - (step / n_steps_per_trajectory)\n",
        "                embedding = center + np.random.randn(128) * (2 + 3 * noise_level)\n",
        "                all_embeddings.append(embedding)\n",
        "                all_labels.append(step)\n",
        "                all_tasks.append(task)\n",
        "    \n",
        "    all_embeddings = np.array(all_embeddings)\n",
        "    \n",
        "    # Apply PCA first for dimensionality reduction (faster)\n",
        "    print(\"Applying PCA...\")\n",
        "    pca = PCA(n_components=50)\n",
        "    embeddings_pca = pca.fit_transform(all_embeddings)\n",
        "    \n",
        "    # Apply t-SNE\n",
        "    print(\"Applying t-SNE (this may take a minute)...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
        "    embeddings_2d = tsne.fit_transform(embeddings_pca)\n",
        "    \n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    \n",
        "    # Plot each task with different colors\n",
        "    for task, color in zip(tasks, colors):\n",
        "        task_mask = np.array(all_tasks) == task\n",
        "        task_embeddings = embeddings_2d[task_mask]\n",
        "        task_labels = np.array(all_labels)[task_mask]\n",
        "        \n",
        "        # Plot points, colored by diffusion step (darker = later step)\n",
        "        scatter = ax.scatter(task_embeddings[:, 0], task_embeddings[:, 1], \n",
        "                            c=task_labels, cmap='viridis', s=30, \n",
        "                            alpha=0.6, edgecolors=color, linewidths=0.5,\n",
        "                            label=task)\n",
        "    \n",
        "    # Add colorbar for diffusion steps\n",
        "    cbar = plt.colorbar(ax.collections[0], ax=ax)\n",
        "    cbar.set_label('Diffusion Step (darker = later)', fontsize=12)\n",
        "    \n",
        "    ax.set_xlabel('t-SNE Dimension 1', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('t-SNE Dimension 2', fontsize=14, fontweight='bold')\n",
        "    ax.set_title('t-SNE Visualization of Denoising Trajectories\\n(Task-Specific Clusters)', \n",
        "                 fontsize=16, fontweight='bold')\n",
        "    ax.legend(loc='upper right', fontsize=11, framealpha=0.9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save as PNG and PDF\n",
        "    output_path_png = figures_dir / \"trajectory_visualization.png\"\n",
        "    output_path_pdf = figures_dir / \"trajectory_visualization.pdf\"\n",
        "    plt.savefig(output_path_png, dpi=300, bbox_inches='tight')\n",
        "    plt.savefig(output_path_pdf, bbox_inches='tight')\n",
        "    print(f\"Saved: {output_path_png}\")\n",
        "    print(f\"Saved: {output_path_pdf}\")\n",
        "    plt.show()\n",
        "\n",
        "plot_trajectory_visualization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "All required figures have been generated and saved to `doc/figures/`:\n",
        "\n",
        "1. ✅ **rank_ablation.png/pdf**: Rank vs. performance and trainable parameters\n",
        "2. ✅ **effective_rank.png/pdf**: Effective rank across diffusion steps\n",
        "3. ✅ **data_efficiency.png/pdf**: Performance vs. training data size\n",
        "4. ✅ **trajectory_visualization.png/pdf**: t-SNE visualization of denoising trajectories\n",
        "\n",
        "**Note**: The current visualizations use example/synthetic data. To use real experimental data:\n",
        "- Update the data loading functions to read from actual experiment outputs\n",
        "- Replace synthetic data arrays with real measurements\n",
        "- Adjust parameters to match your experimental setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook provides tools for analyzing LoRA-Diffusion results. Key analyses:\n",
        "\n",
        "1. **Training curves**: Monitor loss and accuracy over time\n",
        "2. **Method comparison**: Compare different PEFT methods\n",
        "3. **Parameter efficiency**: Analyze trainable parameters vs. performance\n",
        "4. **Step-adaptive ranks**: Visualize rank allocation strategy\n",
        "5. **Multi-task results**: Compare performance across tasks\n",
        "6. **Training time**: Analyze computational efficiency\n",
        "\n",
        "**Next steps:**\n",
        "- Update paths to your actual experiment directories\n",
        "- Run your own analyses\n",
        "- Export results for papers/presentations\n",
        "- Customize visualizations as needed"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
